{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a proposal CSV for one chip against a ground truth CSV\n",
    "\n",
    "This recipe describes how to run evaluation of a proposal CSV for a single chip against a ground truth CSV for the same chip.\n",
    "\n",
    "## Steps\n",
    "1. <a class=\"reference internal\" href=\"#imports\">Imports</a></li>\n",
    "2. <a class=\"reference internal\" href=\"#Load-ground-truth-CSV\">Load ground truth CSV</a></li>\n",
    "3. <a class=\"reference internal\" href=\"#Load-proposal-CSV\">Load proposal CSV</a></li>\n",
    "4. <a class=\"reference internal\" href=\"#Perform-evaluation\">Perform evaluation</a></li>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Imports  \n",
    "\n",
    "For this test case we will only need `cw_eval` installed - [Installation instructions for cw_eval](https://github.com/cosmiq/cw-eval/#installation-instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import cw_eval\n",
    "from cw_eval.baseeval import EvalBase  # class used for evaluation\n",
    "from cw_eval.data import data_dir  # get the path to the sample eval data\n",
    "import pandas as pd  # just for visualizing the outputs in this recipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## Load ground truth CSV\n",
    "\n",
    "We will first instantiate an `EvalBase()` object, which is the core class `cw_eval` uses for comparing predicted labls to ground truth labels. `EvalBase()` takes one argument - the path to the CSV or .geojson ground truth label object. It can alternatively accept a pre-loaded `GeoDataFrame` of ground truth label geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvalBase sample_truth.csv"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_path = os.path.join(data_dir, 'sample_truth.csv')\n",
    "\n",
    "eval_object = EvalBase(ground_truth_path)\n",
    "eval_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, `eval_object` has the following attributes:\n",
    "\n",
    "- `ground_truth_fname`: the filename corresponding to the ground truth data. This is simply `'GeoDataFrame'` if a GDF was passed during instantiation.\n",
    "\n",
    "- `ground_truth_GDF`: GeoDataFrame-formatted geometries for the ground truth polygon labels.\n",
    "\n",
    "- `ground_truth_GDF_Edit`: A deep copy of `eval_object.ground_truth_GDF` which is edited during the process of matching ground truth label polygons to proposals.\n",
    "\n",
    "- `ground_truth_sindex`: The RTree/libspatialindex spatial index for rapid spatial referencing.\n",
    "\n",
    "- `proposal_GDF`: An _empty_ GeoDataFrame instantiated to hold proposals later.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load proposal CSV\n",
    "\n",
    "Next we will load in the proposal CSV file. Note that the `proposalCSV` flag must be set to true for CSV data. If the CSV contains confidence column(s) that indicate confidence in proprosals, the name(s) of the column(s) should be passed as a list of strings with the `conf_field_list` argument; because no such column exists in this case, we will simply pass `conf_field_list=[]`. There are additional arguments available (see [the method documentation](https://cw-eval.readthedocs.io/en/dev/#cw_eval.baseeval.EvalBase.load_proposal)) which can be used for multi-class problems; those will be covered in another recipe. The defaults suffice for single-class problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals_path = os.path.join(data_dir, 'sample_preds.csv')\n",
    "eval_object.load_proposal(proposals_path, proposalCSV=True, conf_field_list=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## Perform evaluation\n",
    "\n",
    "Evaluation iteratively steps through the proposal polygons in `eval_object.proposal_GDF` and determines if any of the polygons in `eval_object.ground_truth_GDF_Edit` have IoU overlap > `miniou` (see [the method documentation](https://cw-eval.readthedocs.io/en/dev/#cw_eval.baseeval.EvalBase.eval_iou)) with that proposed polygon. If one does, that proposal polygon is scored as a true positive. The matched ground truth polygon with the highest IoU (in case multiple had IoU > `miniou`) is removed from `eval_object.ground_truth_GDF_Edit` so it cannot be matched against another proposal. If no ground truth polygon matches with IoU > `miniou`, that proposal polygon is scored as a false positive. After iterating through all proposal polygons, any remaining ground truth polygons in `eval_object.ground_truth_GDF_Edit` are scored as false negatives.\n",
    "\n",
    "There are several additional arguments to this method related to multi-class evaluation which will be covered in a later recipe. See [the method documentation](https://cw-eval.readthedocs.io/en/dev/#cw_eval.baseeval.EvalBase.eval_iou) for usage.\n",
    "\n",
    "The prediction outputs a `list` of `dict`s for each class evaluated (only one `dict` in this single-class case). The `dict`(s) have the following keys:\n",
    "\n",
    "- `'class_id'`: The class being scored in the dict, `'all'` for single-class scoring.\n",
    "\n",
    "- `'iou_field'`: The name of the column in `eval_object.proposal_GDF` for the IoU score for this class. See [the method documentation](https://cw-eval.readthedocs.io/en/dev/#cw_eval.baseeval.EvalBase.eval_iou) for more information.\n",
    "\n",
    "- `'TruePos'`: The number of polygons in `eval_object.proposal_GDF` that matched a polygon in `eval_object.ground_truth_GDF_Edit`.\n",
    "\n",
    "- `'FalsePos'`: The number of polygons in `eval_object.proposal_GDF` that had no match in `eval_object.ground_truth_GDF_Edit`.\n",
    "\n",
    "- `'FalseNeg'`: The number of polygons in `eval_object.ground_truth_GDF_Edit` that had no match in `eval_object.proposal_GDF`.\n",
    "\n",
    "- `'Precision'`: The [precision statistic](https://en.wikipedia.org/wiki/Precision_and_recall) for IoU between the proposals and the ground truth polygons.\n",
    "\n",
    "- `'Recall'`: The [recall statistic](https://en.wikipedia.org/wiki/Precision_and_recall) for IoU between the proposals and the ground truth polygons.\n",
    "\n",
    "- `'F1Score'`: Also known as the [SpaceNet Metric](https://medium.com/the-downlinq/the-spacenet-metric-612183cc2ddb), the [F<sub>1</sub> score](https://en.wikipedia.org/wiki/F1_score) for IoU between the proposals and the ground truth polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "151it [00:00, 240.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'class_id': 'all',\n",
       "  'iou_field': 'iou_score_all',\n",
       "  'TruePos': 151,\n",
       "  'FalsePos': 0,\n",
       "  'FalseNeg': 0,\n",
       "  'Precision': 1.0,\n",
       "  'Recall': 1.0,\n",
       "  'F1Score': 1.0}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_object.eval_iou(calculate_class_scores=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the score is perfect because the polygons in the ground truth CSV and the proposal CSV are identical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (spacenet)",
   "language": "python",
   "name": "spacenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
